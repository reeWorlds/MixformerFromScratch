{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH52jJLM0gSL"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning > /dev/null 2>&1\n",
        "!pip install einops > /dev/null 2>&1\n",
        "!pip install timm > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf MixformerFromScratch\n",
        "!git clone https://github.com/reeWorlds/MixformerFromScratch.git\n",
        "!pip install -e \"MixformerFromScratch\"\n",
        "\n",
        "import site\n",
        "site.main()"
      ],
      "metadata": {
        "id": "pr7XcHkP0xrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  import os\n",
        "  os._exit(0)"
      ],
      "metadata": {
        "id": "IWKInEq50x1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import seaborn as sns\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "from timm.models.layers import DropPath\n",
        "\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "from Mixformer import st2_xformer\n",
        "from Mixformer import st3_mxformer"
      ],
      "metadata": {
        "id": "6Iyeirfc0yGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_prefix = '/content/drive/My Drive/Data/DiplomeGenerated/Stage3'"
      ],
      "metadata": {
        "id": "tSIFjGxJ0yRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder_path = data_prefix\n",
        "\n",
        "train_patches_nums = list(range(10)) # up to 10\n",
        "valid_pathch_num = 10\n",
        "\n",
        "trn_s, trn_t, trn_m = None, None, None\n",
        "vld_s, vld_t, vld_m = None, None, None\n",
        "\n",
        "def get_tensor_by_path(file_path, size, shape, dtype):\n",
        "  mmapped_array = np.memmap(file_path, dtype=dtype, mode='r', shape=(size,))\n",
        "  tensor = torch.from_numpy(mmapped_array)\n",
        "  return tensor.reshape(*shape)\n",
        "\n",
        "def get_data_by_num(path_num):\n",
        "  s_path = os.path.join(data_folder_path, f'patch{path_num}_search.bin')\n",
        "  s_shape = (10000, 64, 64, 3)\n",
        "  s = get_tensor_by_path(s_path, reduce(mul, s_shape), s_shape, np.float32)\n",
        "  t_path = os.path.join(data_folder_path, f'patch{path_num}_target.bin')\n",
        "  t_shape = (10000, 48, 48, 3)\n",
        "  t = get_tensor_by_path(t_path, reduce(mul, t_shape), t_shape, np.float32)\n",
        "  m_path = os.path.join(data_folder_path, f'patch{path_num}_mask.bin')\n",
        "  m_shape = (10000, 64, 64)\n",
        "  m = get_tensor_by_path(m_path, reduce(mul, m_shape), m_shape, np.float32)\n",
        "  return s, t, m\n",
        "\n",
        "list_s, list_t, list_m = [], [], []\n",
        "\n",
        "for patch_num in train_patches_nums:\n",
        "  s, t, m = get_data_by_num(patch_num)\n",
        "  list_s.append(s)\n",
        "  list_t.append(t)\n",
        "  list_m.append(m)\n",
        "  if patch_num % 2 == 0:\n",
        "    print(f'Finished patch_num = {patch_num}')\n",
        "\n",
        "trn_s = torch.cat(list_s, dim=0)\n",
        "trn_t = torch.cat(list_t, dim=0)\n",
        "trn_m = torch.cat(list_m, dim=0)\n",
        "\n",
        "vld_s, vld_t, vld_m = get_data_by_num(valid_pathch_num)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(f'train data shapes are s:{trn_s.shape} t:{trn_t.shape} m:{trn_m.shape}')\n",
        "print(f'train data shapes are s:{vld_s.shape} t:{vld_t.shape} m:{vld_m.shape}')"
      ],
      "metadata": {
        "id": "Oh5ea9ye1BAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(img_s, img_t, img_msk, idx):\n",
        "  plt.clf()\n",
        "  img_s_np = img_s[idx].numpy()\n",
        "  img_t_np = img_t[idx].numpy()\n",
        "  img_m_np = img_msk[idx].numpy()\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(7, 3))\n",
        "  ax[0].imshow(img_s_np)\n",
        "  ax[0].set_title('Search')\n",
        "  ax[1].imshow(img_t_np)\n",
        "  ax[1].set_title('Target')\n",
        "  ax[2].imshow(img_m_np, cmap='gray', vmin=0, vmax=1)\n",
        "  ax[2].set_title('Mask')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "GL4pYRb60yZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "plot_image(trn_s, trn_t, trn_m, idx)"
      ],
      "metadata": {
        "id": "GT_cnjHf0yiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, _s, _t, _m):\n",
        "        self._s = _s\n",
        "        self._t = _t\n",
        "        self._m = _m\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._s)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self._s[idx], self._t[idx], self._m[idx]"
      ],
      "metadata": {
        "id": "ZLZdt-uw0yre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_str = 'large'"
      ],
      "metadata": {
        "id": "0AtbhN4ICUy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningBaseModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    config = st2_xformer.make_transformer_config(size_str)\n",
        "    self.model = st2_xformer.Transformer(config)\n",
        "\n",
        "  def forward(self, _d):\n",
        "    return self.model(_d)"
      ],
      "metadata": {
        "id": "_DBJpepaAzip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningMixFormer(pl.LightningModule):\n",
        "  def __init__(self, base_model=None):\n",
        "    super().__init__()\n",
        "    config = st3_mxformer.make_mixformer_config(size_str)\n",
        "    if base_model is None:\n",
        "      self.model = st3_mxformer.MixFormer(config)\n",
        "    else:\n",
        "      self.model = st3_mxformer.MixFormer(config, base_model)\n",
        "    self.start_lr = 1e-3\n",
        "    self.lr_gamma = 0.75\n",
        "\n",
        "  def forward(self, _s, _t):\n",
        "    return self.model(_s, _t)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    _s, _t, _m_ref = batch\n",
        "    _m_pred = self.model(_s, _t)\n",
        "    loss = F.mse_loss(_m_pred, _m_ref)\n",
        "    self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    _s, _t, _m_ref = batch\n",
        "    _m_pred = self.model(_s, _t)\n",
        "    loss = F.mse_loss(_m_pred, _m_ref)\n",
        "    self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.start_lr, weight_decay=1e-6)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=self.lr_gamma)\n",
        "    return {'optimizer': optimizer,\n",
        "            'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch', 'frequency': 1} }\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_dataset = MyDataset(trn_s, trn_t, trn_m)\n",
        "    return DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    valid_dataset = MyDataset(vld_s, vld_t, vld_m)\n",
        "    return DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-AwrfxcB0y4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trainer(max_epochs):\n",
        "  checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='my_model/',\n",
        "                                        filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "                                        save_top_k=5, mode='min')\n",
        "  csv_logger = pl_loggers.CSVLogger('logs')\n",
        "  trainer = pl.Trainer(max_epochs=max_epochs,callbacks=[checkpoint_callback], logger=csv_logger)\n",
        "  return trainer"
      ],
      "metadata": {
        "id": "zTqRecxM0zHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_prefix = '/content/drive/My Drive/Data/DiplomeGenerated/Stage2'\n",
        "base_model_path = os.path.join(base_model_prefix, f'models/model_{size_str}.ckpt')\n",
        "base_model = LightningBaseModel.load_from_checkpoint(base_model_path)"
      ],
      "metadata": {
        "id": "k1UeC3lSAf_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LightningMixFormer(base_model.model)"
      ],
      "metadata": {
        "id": "FgXUsoye0zgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stages = [1, 2]\n",
        "\n",
        "for stage in stages:\n",
        "  if stage == 1:\n",
        "    trainer = get_trainer(1)\n",
        "    model.start_lr = 5e-4\n",
        "    model.lr_gamma = 0.8\n",
        "    model.model.set_base_requires_grad(False)\n",
        "    trainer.fit(model)\n",
        "  elif stage == 2:\n",
        "    trainer = get_trainer(5)\n",
        "    model.start_lr = 1e-3\n",
        "    model.lr_gamma = 0.75\n",
        "    model.model.set_base_requires_grad(True)\n",
        "    trainer.fit(model)"
      ],
      "metadata": {
        "id": "y4JD_hvs0zp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_prefix = '/content/drive/My Drive/Data/DiplomeGenerated/Stage3'\n",
        "\n",
        "trainer.save_checkpoint(\"model.ckpt\")\n",
        "checkpoint_path = os.path.join(model_prefix, f'models/model_{size_str}.ckpt')\n",
        "trainer.save_checkpoint(checkpoint_path)"
      ],
      "metadata": {
        "id": "_NuwN-2a0zzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(model_prefix, f'models/model_{size_str}.ckpt')\n",
        "model = LightningMixFormer.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
        "model = model.eval().to('cuda')"
      ],
      "metadata": {
        "id": "xk7gUNF80z-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masks(searches, targets, outs):\n",
        "  dataset = MyDataset(searches, targets, outs)\n",
        "  data_loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=False, num_workers=2)\n",
        "  all_masks = []\n",
        "  with torch.no_grad():\n",
        "      for batch in data_loader:\n",
        "          _search = batch[0].to('cuda')\n",
        "          _target = batch[1].to('cuda')\n",
        "          masks = model(_search, _target)\n",
        "          masks = torch.clamp(masks, min=0, max=1)\n",
        "          all_masks.append(masks.cpu())\n",
        "  all_masks = torch.cat(all_masks, dim=0)\n",
        "  return all_masks\n",
        "\n",
        "valid_model_masks = get_masks(vld_s, vld_t, vld_m)\n",
        "#valid_model_masks = get_masks(trn_s[0:1000], trn_t[0:1000], trn_m[0:1000])\n",
        "print(valid_model_masks.shape)"
      ],
      "metadata": {
        "id": "MFb08phv00JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image2(ss, tt, mm, mm_pred, index):\n",
        "  plt.clf()\n",
        "  img_search = ss[index]\n",
        "  img_search_np = img_search.numpy()\n",
        "  img_target = tt[index]\n",
        "  img_target_np = img_target.numpy()\n",
        "  img_out = mm[index]\n",
        "  img_out_np = img_out.numpy()\n",
        "  img_out_pred = mm_pred[index]\n",
        "  img_out_pred_np = img_out_pred.numpy()\n",
        "  fig, ax = plt.subplots(2, 2, figsize=(6, 6))\n",
        "  ax[0,0].imshow(img_search_np)\n",
        "  ax[0,0].set_title('Search Image')\n",
        "  ax[0,1].imshow(img_target_np)\n",
        "  ax[0,1].set_title('Target Image')\n",
        "  ax[1,0].imshow(img_out_np, cmap='gray', vmin=0, vmax=1)\n",
        "  ax[1,0].set_title('Mask')\n",
        "  ax[1,1].imshow(img_out_pred_np, cmap='gray', vmin=0, vmax=1)\n",
        "  ax[1,1].set_title('Predicted Mask')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3OnOTzr100UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind = 2\n",
        "plot_image2(vld_s, vld_t, vld_m, valid_model_masks, ind)\n",
        "#plot_image2(trn_s, trn_t, trn_m, valid_model_masks, ind)"
      ],
      "metadata": {
        "id": "DkCxAR14Dx-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = F.mse_loss(valid_model_masks, vld_m)\n",
        "#loss = F.mse_loss(valid_model_masks, trn_m[0:1000])\n",
        "print(f\"loss = {loss}\")"
      ],
      "metadata": {
        "id": "B2toQqD-OzJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "if True:\n",
        "  try:\n",
        "    shutil.rmtree(\"/content/logs\")\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    shutil.rmtree(\"/content/my_model\")\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "9DDl3_96DyLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}